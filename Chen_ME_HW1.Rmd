---
title: "Marketing Engineering HW1"
author: "Chuhan Chen"
date: "2022/9/28"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1: Climate Change

## a. Linear Regression for Temperature Change Prediction

```{r lr_climate}
climate = read.csv('climate_change.csv')
climate_linear = lm(Temp ~ . - Year - Month, data=climate)
summary(climate_linear)
```

As we can see from the regression results, $R^2=0.744$ .

## b. The Correlation of Variables

```{r corr_climate, message=FALSE, warning=FALSE, fig.align='center', fig.dim=c(4,4), fig.cap='Correlation of the Independent Variables'}
library(corrplot)
climate_corr = cor(climate[, 3:10]) # only consider the independent variables
corrplot.mixed(climate_corr, lower.col='black', number.cex=0.7, tl.cex=0.6)
```

As we can see in Figure 1, the variables with correlation higher than 0.7 are

+ `CO2` & `CH4`, `CO2` & `N2O`, `CO2` & `CFC_12`;
+ `CH4` & `N2O`, `CH4` & `CFC_12`;
+ `N2O` & `CFC_12`;
+ `CFC_11` & `CFC_12`.

## c. Exclude Collinearity and Re-Run the Linear Regression

Remove the variables `CO2`, `CH4` and `CFC_12`, and then re-run the linear regression model.

```{r lr_climate_2}
climate_linear2 = lm(Temp ~ . - Year - Month - CO2 - CH4 - CFC_12, data=climate)
summary(climate_linear2)
```

As we can see from the regression results, $R^2=0.7162$ .


# Problem 2: Forecasting Automobile Sales

## a. Train Test Split and Initial Linear Regression

Instructions:

+ Split the data into a training set (containing all observations for 2010-2017) and a test set (containing all observations for 2018);
+ Build an initial linear regression model to predict monthly Wrangler sales with five independent variables: `Year`, `Unemployment.Rate`, `Wrangler.Queries`, `CPI.Energy`, and `CPI.All`.

```{r Wrangler_lr}
WE2018 = read.csv('WranglerElantra2018.csv')
WE_train = WE2018[WE2018$Year <= 2017, ] # training set
WE_test = WE2018[WE2018$Year == 2018, ] # test set
Wrangler_lr1 = lm(
  Wrangler.Sales ~ Year + Unemployment.Rate + Wrangler.Queries + CPI.Energy + CPI.All, 
  data=WE_train)
summary(Wrangler_lr1)
```

Variables `Year`, `Unemployment.Rate`, `Wrangler.Queries` and `CPI.Energy` are statistically significant at the 95% level.

## b. Choose a Subset of the Five Variables to Update the Model

I remove the variable `Year`. The new model is trained and its results are shown below:

```{r Wrangler_lr2}
Wrangler_lr2 = lm(
  Wrangler.Sales ~ Unemployment.Rate + Wrangler.Queries + CPI.Energy + CPI.All, 
  data=WE_train)
summary(Wrangler_lr2)
```

Examine the performance of the new model in the test set, as captured by the $OSR^2$ :

```{r Wrangler_OSR2}
pred = predict(Wrangler_lr2, newdata = WE_test)
SSE = sum((pred - WE_test$Wrangler.Sales)^2)
train_mean = mean(WE_train$Wrangler.Sales)
SST = sum((train_mean - WE_test$Wrangler.Sales)^2)
OSR2 = 1 - SSE/SST
OSR2
```

Memo:

(i) *Why I select these variables:* I remove the variable `Year` because it is a factor, and it should not be included in the linear regression as a numerical.
(ii) *The linear regression equation in the new model:* 
$$
\begin{aligned}
\mathtt{Wrangler.Sales} = & 153441.27 -2780.27 \times \mathtt{Unemployment.Rate} + 270.8 \times \mathtt{Wrangler.Queries} \\
& + 96 \times \mathtt{CPI.Energy} -681.35 \times \mathtt{CPI.All}
\end{aligned}
$$
(iii) *Interpretation of the coefficients for the independent variables:* With the other independent variables unchanged, when an independent variable changes one unit, it affects the dependent variable whose value change equals the coefficient of the independent variable.
(iv) *Which independent variables are statistically significant:* All the four independent variables are statistically significant at the 95% level.
(v) *Do the signs of the model's coefficients make sense:* `Wrangler.Sales` is positively affected by `Wrangler.Queries` and `CPI.Energy`, and negatively affected by `Unemployment.Rate` and `CPI.All`. All the signs of the model's coefficients make sense.
(vi) *Interpret the in-sample R2:* $R^2=0.7998$ . This indicates a good performance of the model to predict Wrangler sales in the training set.
(vii) *Interpret the out-of-sample OSR2:* $OSR^2=0.51$. The performance of the model in the test set is worse than in the training set, but a 0.51 $OSR^2$ still indicates that the model is fairly usable.

## c. Plot `Wrangler.Sales` and `Wrangler.Queries` for Each Month over 2010-2018

The result is shown in Figure 2.

```{r plot Wrangler sales and queries, message=FALSE, warning=FALSE, fig.align='center', fig.dim=c(7,4), fig.cap="Scatter Plot of Wrangler's Sales and Queries"}
library(dplyr)
library(ggplot2)
WE2018$date <- as.Date(WE2018$date,format="%m/%d/%y")
ggplot(data=WE2018, aes(x=date))+
  geom_line(aes(y=Wrangler.Sales, colour = "sales")) +
  geom_line(aes(y=Wrangler.Queries*200, colour = "queries")) +
  scale_y_continuous(sec.axis = sec_axis(~./200, name = "Wrangler Queries")) +
  theme_bw() +
  labs(x = "Month", y = "Wrangler Sales", colour="Parameter") +
  scale_colour_manual(values = c("blue", "red")) +
  theme(legend.position = c(0.1,0.85)) +
  scale_x_date(date_labels = "%Y-%m")
```

The temporal patterns of the two variables: The trends (with respect to months) of the two variables are similar. In other words, when Wrangler's queries are higher, Wrangler's sales are higher, and when Wrangler's queries are lower, Wrangler's sales are lower. This makes intuitive business sense because higher queries imply that people are more interested in this car model, and greater interest will convert to higher sales.

## d. Modeling Seasonality in Linear Regression with `Month.Factor`

```{r Wrangler_lr3}
Wrangler_lr3 = lm(
  Wrangler.Sales ~ Month.Factor + Unemployment.Rate + Wrangler.Queries + CPI.Energy + CPI.All, 
  data=WE_train)
summary(Wrangler_lr3)
```

Examine the performance of the new model in the test set, as captured by the $OSR^2$ :

```{r Wrangler_OSR2_withMonth}
pred = predict(Wrangler_lr3, newdata = WE_test)
SSE = sum((pred - WE_test$Wrangler.Sales)^2)
train_mean = mean(WE_train$Wrangler.Sales)
SST = sum((train_mean - WE_test$Wrangler.Sales)^2)
OSR2 = 1 - SSE/SST
OSR2
```

Memo:

(i) *What changed in the model:* The variable `Month.Factor` captures seasonality, and this can improve the model's performance.
(ii) *The linear regression equation in the new model:* 
$$
\begin{aligned}
  \mathtt{Wrangler.Sales} 
  = & 115688.52 -4185.09 \times \mathtt{January} -3214.28 \times \mathtt{February} -511.23 \times \mathtt{March} \\
  & + 1472.18 \times \mathtt{May} + 222.49 \times \mathtt{June} -208.86 \times \mathtt{July} -276.78 \times \mathtt{August} \\
  & -1302.68 \times \mathtt{September} -1439.10 \times \mathtt{October} -2306.13 \times \mathtt{November} \\
  & -840.83 \times \mathtt{December} -2692.23 \times \mathtt{Unemployment.Rate} \\
  & + 145.82 \times \mathtt{Wrangler.Queries} + 67.86 \times \mathtt{CPI.Energy} -458.46 \times \mathtt{CPI.All}
\end{aligned}
$$
(iii) *Interpretation of the coefficients for each of the `Month.Factor` categorical variables:* Use `April` as the baseline for `Month.Factor`. With the other independent variables unchanged, the coefficient of a level of `Month.Factor` represents the difference of this level and the baseline (`April`) when affecting the dependent variable.
(iv) *Which independent variables are statistically significant:* Variables `January`, `Februrary`, `May`, `September`, `November`, `Unemployment.Rate`, `Wrangler.Queries`, `CPI.Energy` and `CPI.All` are statistically significant at the 95% level.
(v) *Interpret the in-sample R2:* $R^2=0.9088$ . This indicates a good performance of the model to predict Wrangler sales in the training set, and it is much better than the previous model that does not consider seasonality.
(vi) *Interpret the out-of-sample OSR2:* $OSR^2=0.635$ in the test set. This is also better than the previous model.
(vii) *Has adding the independent variable `Month.Factor` improved the quality of the model model:* It has because both of the $R^2$ in the training set and the $OSR^2$ in the test set are greatly improved. As seasonality is an important feature to predict Wrangler's sales, it is necessary to consider this feature in the regression model to achieve a better performance.

## e. Predict the Monthly Sales of Elantra with Linear Regression

```{r Elantra_lr}
Elantra_lr = lm(
  Elantra.Sales ~ Month.Factor + Unemployment.Rate + Wrangler.Queries + CPI.Energy + CPI.All, 
  data=WE_train)
summary(Elantra_lr)
```

Examine the performance of the new model in the test set, as captured by the $OSR^2$ :

```{r Elantra_OSR2_withMonth}
pred = predict(Elantra_lr, newdata = WE_test)
SSE = sum((pred - WE_test$Elantra.Sales)^2)
train_mean = mean(WE_train$Elantra.Sales)
SST = sum((train_mean - WE_test$Elantra.Sales)^2)
OSR2 = 1 - SSE/SST
OSR2
```

As we can see in the regression results, $R^2=0.6437$ in the training set, and $OSR^2=-2.638$ in the test set. The quality of this model is not good because many of the independent variables are not statistically significant at the 95% level, and $OSR^2$ is negative in the test set.

## f. Trends of the Sales and their Correlations

**(i)** Plot `Elantra.Sales` and `Elantra.Queries` for each month over the 2010-2018 period. The result is shown in Figure 3.

```{r plot Elantra sales and queries, fig.align='center', fig.dim=c(7,4), fig.cap="Scatter Plot of Elantra's Sales and Queries"}
ggplot(data=WE2018, aes(x=date))+
  geom_line(aes(y=Elantra.Sales, colour = "sales")) +
  geom_line(aes(y=Elantra.Queries*1000, colour = "queries")) +
  scale_y_continuous(sec.axis = sec_axis(~./1000, name = "Elantra Queries")) +
  theme_bw() +
  labs(x = "Month", y = "Elantra Sales", colour="Parameter") +
  scale_colour_manual(values = c("blue", "red")) +
  theme(legend.position = c(0.1,0.85)) +
  scale_x_date(date_labels = "%Y-%m")
```

**(ii)** Compute correlations of the sales with other independent variables.

Correlations of `Elantra.Sales` with independent variables `Year`, `Unemployment.Rate`, `Elantra.Queries` and `CPI.Energy`:

```{r Elantra sales correlations}
corrE = as.data.frame(
  cor(WE2018[,c("Elantra.Sales","Year","Unemployment.Rate","Elantra.Queries","CPI.Energy")]))
data.frame(Elantra.Sales=corrE$Elantra.Sales, row.names=row.names(corrE))
```

Correlations of `Wrangler.Sales` with independent variables `Year`, `Unemployment.Rate`, `Elantra.Queries` and `CPI.Energy`:

```{r Wrangler sales correlations}
corrW = as.data.frame(
  cor(WE2018[,c("Wrangler.Sales","Year","Unemployment.Rate","Elantra.Queries","CPI.Energy")]))
data.frame(Wrangler.Sales=corrW$Wrangler.Sales, row.names=row.names(corrW))
```

**Comments:** Wrangler's sales are increasing in the long run, and have a seasonal pattern as every year they are the highest in summer and lowest in winter. Elantra's sales do not have a clear trend, and have high volatility. In terms of correlations, Wrangler's sales are highly correlated with the independent variables `Year`, `Unemployment.Rate` and `Elantra.Queries`, whereas Elantra's sales have relatively low correlations with these variables.

The unclear trend and the low correlations make it harder for the Elantra model to predict sales. To improve the quality of this model, we can acquire more data regarding Elantra's sales and other independent variables, or we can add new independent variables to the model.

## g. Make Recommendations and Decisions According to Predictions

Let $y$ denote the expected demand in April, so $\hat{y}=15855$ and $\mathrm{SE}(\hat{y})=1118$ . Suppose the demand is normally distributed, then the 95% confidence interval of the expected demand is
$$
\left [ \hat{y} - z_{\alpha/2} \mathrm{SE}(\hat{y}) \ ,\ \hat{y} + z_{\alpha/2} \mathrm{SE}(\hat{y}) \right ]
= \left [ 15855-1.96\times1118 \ ,\ 15855+1.96\times1118 \right ] = [13663.72\ ,\ 18046.28]
$$

The costs of producing **less** vehicles than demanded are that some consumers are not able to buy the vehicles they want, and the company will lose profit. The costs of producing **more** vehicles than demanded are that some of the vehicles cannot be sold this month and will convert to inventory next month, so the storage costs for inventory will increase.

I think the costs of producing **less** vehicles than demanded are likely to be higher. Therefore, according to the confidence interval calculated above, I recommend producing approximately 18,000 vehicles.


