---
title: "Linear Regression v.s. Ramdom Forest"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Predicting Housing Prices in Ames, Iowa

```{r read data}
ames = read.csv("ames.csv")[, 2:75]
ames$SalePrice = as.numeric(ames$SalePrice)
```

## a. Explore the Dataset

### 1. The sales prices of each year

```{r sample sizes of each year}
table(ames$YrSold)
```

```{r sales price trend, fig.align='center', fig.dim=c(6,3), fig.cap='The Housing Prices in Ames, Iowa over the 2006-10 Period', dev.args=list(pointsize=8)}
boxplot(SalePrice ~ YrSold, data=ames, xlab="Year", ylab="Sale Price")
```

As we can see, the sample sizes of housing prices in 2006 to 2009 are about the same, and the one in 2010 is much smaller. From the box plots in Figure 1, the range of each year's housing prices does not show a big difference.

### 2. Correlation between areas and prices

```{r corr_area_price, message=FALSE, warning=FALSE, fig.align='center', fig.dim=c(3,3), fig.cap='Correlation between Different Types of Areas and Housing Prices', dev.args=list(pointsize=8)}
library(corrplot)
areas_prices = subset(ames, select=c("TotalBsmtSF", "X1stFlrSF", "X2ndFlrSF", 
                                     "GarageArea", "PoolArea", "SalePrice"))
corrplot.mixed(cor(areas_prices), lower.col="black", tl.pos="lt", tl.cex=0.7)
```

From Figure 2, we can see total basement area, first floor area, and garage area have strong positive correlations with housing prices. Second floor area and pool area have weak positive correlations with housing prices.

### 3. Memo

We want to use regression models to predict the housing prices in Ames, Iowa. In the regression models, sales price is the dependent variable and other variables are independent variables. The independent variables are indicators of the housing price, and regression models help us find the patterns of these indicators. The predictions give us insights on how the housing price depends on the attributes of a house. With the predictions, when we are buying a house, we can look into the housing attributes and have a reasonable guess at the housing price.

## b. Linear Regression Model

We first split the train and the test sets. We set training size to 0.8 and test size to 0.2.

```{r split}
set.seed(123)
split = sort(sample(seq_len(nrow(ames)), 0.8*nrow(ames)))
amesTrain = ames[split,]
amesTest = ames[-split,]
```

Then we fit a linear regression model on the training set.

```{r linear regression}
lr_fit = lm(SalePrice ~ ., data=amesTrain)
summary(lr_fit)
```

As we can see from the results of the linear regression model, the number of variables that are statistically significant at the 95% level is 69. 

Some coefficients are reported as "NA". This is because of singularity. For example, the value of `TotalBsmtSF` equals the sum of `BsmtFinSF1`, `BsmtFinSF2`, and `BsmtUfSF`, and this singularity causes the "NA" of the `TotalBsmtSF` coefficient. Other "NA" coefficients are generated because the categorical values in one variable is the same as those in other variables. For example, the "No Basement" level in the variable `BsmtQual` is the same as that in `BsmtCond`, `BsmtExposure`, `BsmtFinType1`, and `BsmtFinType2`, so these levels are useless information and their coefficients turn out to be "NA".

## c. Random Forest Model

We construct a random forest model with 80 trees and a nodesize of 25. We use cross-validation to select the value of the mtry parameter.

```{r random forest, message=FALSE, warning=FALSE}
library(caret)
library(randomForest)
rf.cv = train(y = amesTrain$SalePrice,
              x = subset(amesTrain, select=-c(SalePrice)),
              method="rf", nodesize=25, ntree=80,
              trControl=trainControl(method="cv", number=10),  # 10-fold 
              tuneGrid=data.frame(mtry=seq(1,10,1)))           # tune mtry
rf.cv
```

We inspect the most important variables in the final model. The ten most important variables are shown as below.

```{r rf importance}
rf_fit = rf.cv$finalModel
important_vars = importance(rf_fit)
head(important_vars[order(important_vars, decreasing=TRUE),], 10)
```

The ten most important variables indicate that living areas and the garage matter to the housing price, as variables `GrLivArea`, `GarageArea`, `X1stFlrSF`, and `TotalBsmtSF` are related to areas, and variables `GarageCars`, `GarageArea`, and `GarageYrBlt` are related to the garage.

## d. Report Performance Metrics

For the linear regression model, the in-sample $R^2$, $MAE$ and $RMSE$ and the out-of-sample $R^2$, $MAE$ and $RMSE$ are respectively

```{r linear regression performance, warning=FALSE}
# in sample
in_original = amesTrain$SalePrice
in_predicted = predict(lr_fit, newdata=amesTrain)
in_R2 = R2(in_predicted, in_original)
in_MAE = MAE(in_predicted, in_original)
in_RMSE = RMSE(in_predicted, in_original)
# out of sample
out_original = amesTest$SalePrice
out_predicted = predict(lr_fit, newdata=amesTest)
out_R2 = R2(out_predicted, out_original)
out_MAE = MAE(out_predicted, out_original)
out_RMSE = RMSE(out_predicted, out_original)
data.frame(R2=c(in_R2, out_R2), MAE=c(in_MAE, out_MAE), RMSE=c(in_RMSE, out_RMSE),
           row.names=c("in-sample", "out-of-sample")) # output
```

For the random forest model, the in-sample $R^2$, $MAE$ and $RMSE$ and the out-of-sample $R^2$, $MAE$ and $RMSE$ are respectively

```{r random forest performance, warning=FALSE}
# in sample
in_original = amesTrain$SalePrice
in_predicted = predict(rf_fit, newdata=amesTrain)
in_R2 = R2(in_predicted, in_original)
in_MAE = MAE(in_predicted, in_original)
in_RMSE = RMSE(in_predicted, in_original)
# out of sample
out_original = amesTest$SalePrice
out_predicted = predict(rf_fit, newdata=amesTest)
out_R2 = R2(out_predicted, out_original)
out_MAE = MAE(out_predicted, out_original)
out_RMSE = RMSE(out_predicted, out_original)
data.frame(R2=c(in_R2, out_R2), MAE=c(in_MAE, out_MAE), RMSE=c(in_RMSE, out_RMSE),
           row.names=c("in-sample", "out-of-sample")) # output
```

## e. Recommend One Model

Based on the six performance metrics calculated in question d, I recommend the linear regression model to predict the housing prices. In this case, random forest has better in-sample performance, and linear regression has better out-of-sample performance. The differences between the in-sample and out-of-sample $R^2$, $MAE$, and $RMSE$ of the linear regression model are smaller than those of the random forest model, suggesting that the linear regression model generally fits better and is more robust when doing out-of-sample predictions. In addition, linear regression is more interpretable than random forest as it gives estimated parameters on the coefficients of the predictors. Linear regression is a less complex model than random forest, but it does not necessarily mean linear regression will not have a performance as good as random forest.


